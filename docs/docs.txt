Directory structure:
â””â”€â”€ linkcovery/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ main.py
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ .env.example
    â”œâ”€â”€ .python-version
    â”œâ”€â”€ app/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ cli/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â””â”€â”€ commands/
    â”‚   â”‚       â”œâ”€â”€ import_export.py
    â”‚   â”‚       â”œâ”€â”€ link.py
    â”‚   â”‚       â””â”€â”€ user.py
    â”‚   â””â”€â”€ core/
    â”‚       â”œâ”€â”€ logger.py
    â”‚       â”œâ”€â”€ settings.py
    â”‚       â”œâ”€â”€ utils.py
    â”‚       â”œâ”€â”€ database/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ crud.py
    â”‚       â”‚   â”œâ”€â”€ models.py
    â”‚       â”‚   â””â”€â”€ repositories.py
    â”‚       â””â”€â”€ services/
    â”‚           â””â”€â”€ import_export/
    â”‚               â”œâ”€â”€ exporter.py
    â”‚               â””â”€â”€ importer.py
    â””â”€â”€ docs/

================================================
File: README.md
================================================
# Linkcovery

![Linkcovery Logo](https://via.placeholder.com/150)

**Linkcovery** is a powerful bookmark and link discovery tool built with Python, designed to help users efficiently manage and explore their collection of links. It provides an intuitive command-line interface (CLI) that enables developers, researchers, and avid internet users to seamlessly add, search, and organize links.

## Table of Contents

- [Features](#features)
- [Installation](#installation)
- [Configuration](#configuration)
- [Usage](#usage)
  - [User Commands](#user-commands)
  - [Link Commands](#link-commands)
  - [Import Commands](#import-commands)
- [Development](#development)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)

## Features

- **User Management**: Easily create, update, delete, and list users.
- **Link Management**: Add, list, search, update, and delete links with full metadata (URL, description, domain, tags).
- **Search Capabilities**: Advanced search functionality to filter links by domain, tags, description, read status, and more.
- **Link Import**: Import links from `.txt`, `.csv`, and `.json` files, associating them with a specific user.
- **Atomic Operations**: Ensures data integrity during user and link creation with atomic transactions.
- **Rich CLI Interface**: A user-friendly, interactive CLI with prompts and colored output for enhanced usability.
- **SQLite Database**: Stores data efficiently in an SQLite database, supporting connection pooling for optimized performance.

## Installation

### Prerequisites

- **Python 3.13+**: Ensure Python 3.13 or higher is installed on your system.
- **UV**: Used for dependency management and packaging.

### Steps

1. **Clone the Repository**

   ```bash
   git clone https://github.com/yourusername/linkcovery.git
   cd linkcovery
   ```

2. **Install Dependencies**

   Using UV:

   ```bash
   uv sync
   ```

3. **Set Up Environment Variables**

   Copy the example environment file and configure it:

   ```bash
   cp .env.example .env
   ```

   Edit the `.env` file to set your preferred configurations.

4. **Run the Application**

   Before running the app, initialize the SQLite database:

   ```bash
   uv run linkcovery --help
   ```

## Configuration

Linkcovery uses environment variables for configuration. The `.env` file can be configured with the following settings:

1. **Environment Variables**

   Create a `.env` file in the root directory based on `.env.example`:

   ```bash
   cp .env.example .env
   ```

2. **Configure `.env`**

   Example configuration:

   ```env
   DATABASE_NAME=app.db
   DEBUG=True
   ALLOW_EXTENTIONS=csv,txt,json
   ```

   - `DATABASE_NAME`: Name of the SQLite database file.
   - `DEBUG`: Enable or disable debug mode.
   - `ALLOW_EXTENSIONS`: Allowed file extensions for importing links.

## Usage

Linkcovery provides a CLI built using [Typer](https://typer.tiangolo.com/) to manage users and links.

### Running the CLI

Activate the virtual environment and run the CLI:

```bash
uv run
```

Alternatively, you can use Poetry to run commands without activating the shell:

```bash
uv run linkcovery [COMMAND]
```

### User Commands

#### Add a New User

```bash
linkcovery user create --name "Alice" --email "alice@example.com"
```

#### List All Users

```bash
linkcovery user list
```

### Link Commands

#### Add a New Link

```bash
linkcovery link create --url "https://example.com" --domain "example.com" --author-email "alice@example.com" --description "An example website" --tag "example" "test"
```

#### Search for Links

```bash
linkcovery link search --domain "example" --tag "test" --description "example" --sort-by "created_at" --sort-order "DESC" --limit 5 --offset 0
```

#### Delete a Link

```bash
linkcovery link delete --link-id 1
```

#### Update a Link

```bash
linkcovery link update --link-id 1 --description "Updated description" --is-read True
```

### Import Commands

#### Import Links from a File

Import links from `.txt`, `.csv`, or `.json` files.

```bash
linkcovery db import --file-path links.txt --author-id 1
```

## License

[MIT License](LICENSE)

## Contact

For inquiries or support, please contact:

- **Email**: [arian24b@gmail.com](mailto:arian24b@gmail.com)
- **GitHub**: [@arian24b](https://github.com/arian24b)

---

_Made with â¤ï¸ and Python ðŸ_


================================================
File: main.py
================================================
from app.cli import cli_app

if __name__ == "__main__":
    cli_app()


================================================
File: pyproject.toml
================================================
[project]
name = "linkcovery"
version = "0.3.0"
description = "Bookmark and Link discovery tool for people with love and Python :)"
readme = "README.md"
authors = [{name = "Arian Omrani"}]
requires-python = ">=3.13"
dependencies = [
    "pydantic[email]>=2.10.6",
    "pydantic-settings>=2.7.1",
    "rich>=13.9.4",
    "sqlalchemy>=2.0.38",
    "typer>=0.15.1",
]

[dependency-groups]
dev = [
    "alembic>=1.14.1",
    "pytest>=8.3.4",
    "sqlite-web>=0.6.4",
]

[tool.ruff]
line-length = 120

[project.urls]
Homepage = "https://github.com/arian24b/linkcovery"
Issues = "https://github.com/arian24b/linkcovery/issues"

[project.scripts]
linkcovery = "app.cli:cli_app"

[tool.uv]
package = true


================================================
File: .env.example
================================================
DATABASE_NAME=app.db
DEBUG=False


================================================
File: .python-version
================================================
3.13


================================================
File: app/cli/__init__.py
================================================
from typer import Typer

from app.cli.commands import user, link, import_export
from app.core.settings import settings


# Initialize Typer for potential future CLI enhancements
cli_app = Typer(
    name=settings.APP_NAME,
    no_args_is_help=True,
    help=f"{settings.APP_NAME}, Bookmark management CLI Application",
)

cli_app.add_typer(user.app, name="user")
cli_app.add_typer(link.app, name="link")
cli_app.add_typer(import_export.app, name="db")


================================================
File: app/cli/commands/import_export.py
================================================
from typer import Typer, Option, Exit
from os import path
from pathlib import Path

from app.core.logger import AppLogger
from app.core.utils import check_file
from app.core.database import user_service
from app.core.services.import_export.importer import txt_import, csv_import, json_import
from app.core.services.import_export.exporter import (
    export_users_to_json,
    export_users_to_csv,
    export_links_to_json,
    export_links_to_csv,
)

logger = AppLogger(__name__)
app = Typer()


@app.command("import", help="Import links from a TXT, CSV, or JSON file.")
def import_links(
    file_path: str = Option(..., help="Path to the file to import."),
    author_id: int = Option(..., help="ID of the author to associate with the imported links."),
) -> None:
    if not (author := user_service.get_user(user_id=author_id)):
        logger.error(f"Author with ID '{author_id}' does not exist.")
        raise Exit(code=1)

    try:
        check_file(file_path)
    except Exception as e:
        logger.error(f"Error checking file: {e}")
        return
    extension = path.splitext(file_path)[1].lower()

    if extension == ".txt":
        txt_import(file_path, author.id)
    elif extension == ".csv":
        csv_import(file_path, author.id)
    elif extension == ".json":
        json_import(file_path, author.id)
    else:
        logger.error(f"Unsupported file extension: {extension}")


@app.command("export-users", help="Export all users to a JSON or CSV file.")
def export_users(
    format: str = Option("json", "--format", "-f", help="Export format: json or csv", show_default=True),
    output: str = Option("users_export.json", "--output", "-o", help="Output file path", show_default=True),
) -> None:
    format = format.lower()
    try:
        if format == "json":
            export_users_to_json(output)
        elif format == "csv":
            export_users_to_csv(output)
        else:
            logger.error(f"Unsupported export format: {format}. Choose 'json' or 'csv'.")
            raise Exit(code=1)
    except Exception as e:
        logger.error(f"Error exporting users: {e}")
        raise Exit(code=1)


@app.command("export-links", help="Export links to a JSON or CSV file. Optionally filter by author ID.")
def export_links(
    format: str = Option("json", "--format", "-f", help="Export format: json or csv", show_default=True),
    output: str = Option("links_export.json", "--output", "-o", help="Output file path", show_default=True),
    author_id: int | None = Option(
        None, "--author-id", "-a", help="Filter links by author ID. If not provided, exports all links."
    ),
) -> None:
    format = format.lower()
    try:
        if format == "json":
            export_links_to_json(output, author_id)
        elif format == "csv":
            export_links_to_csv(output, author_id)
        else:
            logger.error(f"Unsupported export format: {format}. Choose 'json' or 'csv'.")
            raise Exit(code=1)
    except Exception as e:
        logger.error(f"Error exporting links: {e}")
        raise Exit(code=1)


@app.command("export-all", help="Export all users and links to JSON or CSV files.")
def export_all_command(
    format: str = Option(
        "json", "--format", "-f", help="Export format for both users and links: json or csv", show_default=True
    ),
    output_dir: str | None = Option(None, "--output-dir", "-d", help="Directory to store exported files."),
    author_id: int | None = Option(
        None, "--author-id", "-a", help="Filter links by author ID for export. If not provided, exports all links."
    ),
) -> None:
    format = format.lower()
    if format not in {"json", "csv"}:
        logger.error(f"Unsupported export format: {format}. Choose 'json' or 'csv'.")
        raise Exit(code=1)

    if not output_dir:
        output_dir = Path.cwd()
    else:
        output_dir = Path(output_dir)
        try:
            output_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            logger.error(f"Failed to create directory '{output_dir}': {e}")
            raise Exit(code=1)

    users_output = output_dir / f"users_export.{format}"
    links_output = output_dir / f"links_export.{format}"

    try:
        export_users_to_json(str(users_output))
        export_links_to_json(str(links_output), author_id)
        logger.info(f"Exported all data successfully to '{users_output}' and '{links_output}'.")
    except Exception as e:
        logger.error(f"Error exporting all data: {e}")
        raise Exit(code=1)


================================================
File: app/cli/commands/link.py
================================================
from typer import Typer, Option, Exit, prompt

from app.core.logger import AppLogger
from app.core.database import user_service, link_service

logger = AppLogger(__name__)
app = Typer()


@app.command(help="Add a new link to the database.")
def create(
    url: str | None = Option(None, help="URL of the link."),
    domain: str | None = Option(None, help="Domain of the link."),
    author_email: str | None = Option(None, help="Email of the author."),
    description: str | None = Option("", help="Description of the link."),
    tags: list[str] = Option([], "--tag", "-t", help="Tags associated with the link."),
    is_read: bool = Option(False, "--is-read", "-r", help="Mark the link as read or unread."),
) -> None:
    if not url:
        url = prompt("URL of the link")
    if not domain:
        domain = prompt("Domain of the link")
    if not author_email:
        author_email = prompt("Author's email")
    if not (user := user_service.get_user(user_email=author_email)):
        logger.error(f"Author with email '{author_email}' does not exist.")
        raise Exit(code=1)

    link_id = link_service.create_link(
        url=url,
        description=description,
        domain=domain,
        tag=", ".join(tags) if isinstance(tags, list) else tags,
        author_id=user.id,
        is_read=is_read,
    )
    if link_id:
        logger.info(f"Link added with ID: {link_id}")
    else:
        logger.error("Failed to add link.")


@app.command(help="List all links with their authors.")
def list_link() -> None:
    if not (links := link_service.get_links()):
        logger.warning("No links found.")
        return

    for link in links:
        logger.info(f"ID: {link.id}, URL: {link.url}, Domain: {link.domain}, Author: {link.author}")


@app.command(help="Search for links based on various filters.")
def search(
    domain: str | None = Option(None, help="Filter by domain."),
    tags: list[str] = Option([], "--tag", "-t", help="Tags to filter by."),
    description: str | None = Option(None, help="Filter by description."),
    sort_by: str | None = Option(None, help="Field to sort by (e.g. created_at, updated_at, domain)."),
    sort_order: str = Option("ASC", help="Sort order: ASC or DESC."),
    limit: int = Option(3, help="Number of results to return."),
    offset: int = Option(0, help="Number of results to skip."),
    is_read: bool | None = Option(None, help="Filter by read status."),
) -> None:
    criteria = {
        "domain": domain,
        "tag": tags,
        "description": description,
        "sort_by": sort_by,
        "sort_order": sort_order,
        "limit": limit,
        "offset": offset,
        "is_read": is_read,
    }
    criteria = {k: v for k, v in criteria.items() if v not in [None, [], ""]}
    results = link_service.search_links(criteria)
    if not results:
        logger.warning("No matching links found.")
        return
    for link in results:
        logger.info(
            f"ID: {link.id}, URL: {link.url}, Domain: {link.domain}, "
            f"Description: {link.description}, Tags: {link.tag}, Read: {link.is_read}"
        )


@app.command(help="Delete a link by its ID.")
def delete(link_id: int = Option(..., help="ID of the link to delete.")) -> None:
    if link_service.delete_link(link_id):
        logger.info(f"Link with ID {link_id} has been deleted.")
    else:
        logger.error(f"Failed to delete link with ID {link_id}.")


@app.command(help="Update a link's details by its ID.")
def update(
    link_id: int = Option(..., help="ID of the link to update."),
    url: str | None = Option(None, help="New URL of the link."),
    domain: str | None = Option(None, help="New domain of the link."),
    description: str | None = Option(None, help="New description of the link."),
    tags: list[str] | None = Option(None, "--tag", "-t", help="New tags for the link."),
    is_read: bool | None = Option(None, "--is-read", "-r", help="Mark as read or unread."),
) -> None:
    # Check if link exists
    if not link_service.get_link(link_id):
        logger.error(f"No link found with ID {link_id}.")
        raise Exit(code=1)

    # Collect data to update
    update_data = {}
    if url:
        update_data["url"] = url
    if domain:
        update_data["domain"] = domain
    if description is not None:
        update_data["description"] = description
    if tags is not None:
        update_data["tag"] = ", ".join(tags) if isinstance(tags, list) else tags
    if is_read is not None:
        update_data["is_read"] = is_read

    # Perform the update
    if link_service.update_link(link_id, **update_data):
        logger.info(f"Link with ID {link_id} has been updated.")
    else:
        logger.error(f"Failed to update link with ID {link_id}.")


@app.command("read-link", help="Mark 3 links as read for a given author.")
def mark_links_as_read(author_id: int = Option(..., help="ID of the author")) -> None:
    if not (links := link_service.get_links_by_author(author_id=author_id, number=3)):
        logger.warning("No links found to update.")
        return

    link_ids = [link.id for link in links if link.id is not None]
    link_service.update_is_read_for_links(link_ids)

    for link in links:
        logger.info(f"Marked link {link.id} as read: {link.url}")


================================================
File: app/cli/commands/user.py
================================================
from typer import Typer, Option
from rich.table import Table

from app.core.logger import AppLogger
from app.core.database import user_service

logger = AppLogger(__name__)
app = Typer()


@app.command(name="create", help="Create a new user with the specified name and email.")
def create(name: str = Option(..., prompt=True), email: str = Option(..., prompt=True)):
    try:
        user = user_service.create_user({"name": name, "email": email})
        logger.print(f"{name}, your account has been created with ID: {user.id} and Email: {email}")
    except Exception as e:
        logger.error(f"Error creating user: {e}")


@app.command(name="read", help="Fetch a user by ID.")
def read_user(user_id: int):
    if not (user := user_service.get_user(user_id)):
        logger.error(f"No user found with ID {user_id}")
        return
    table = Table(title="User")
    table.add_column("ID", style="cyan")
    table.add_column("Name", style="magenta")
    table.add_column("Email", style="green")
    table.add_row(str(user.id), user.name, user.email)
    logger.print(table)


@app.command()
def update(user_id: int, name: str = Option(None, prompt=True), email: str = Option(None, prompt=True)):
    update_data = {}
    if name:
        update_data["name"] = name
    if email:
        update_data["email"] = email

    if not update_data:
        logger.warning("No updates provided.")
        return

    if user_service.update_user(user_id, update_data):
        logger.print(f"User with ID: {user_id} updated successfully")
    else:
        logger.error(f"User with ID: {user_id} not found.")


@app.command()
def delete(user_id: int):
    user_service.delete_user(user_id)
    logger.print(f"User with ID: {user_id} deleted")


@app.command()
def list():
    table = Table(title="Users")
    table.add_column("ID", style="cyan")
    table.add_column("Name", style="magenta")
    table.add_column("Email", style="green")

    if not (users := user_service.get_users()):
        logger.print("No users found.")
        return
    for user in users:
        table.add_row(str(user.id), user.name, user.email)
    logger.print(table)


================================================
File: app/core/logger.py
================================================
from logging import getLogger, Formatter, DEBUG, INFO
from rich.console import Console
from rich.logging import RichHandler

from .settings import settings


class AppLogger:
    def __init__(self, name: str):
        self.console = Console()
        self.logger = getLogger(name)
        self.logger.setLevel(DEBUG if settings.DEBUG else INFO)
        log_handler = RichHandler(
            show_time=settings.DEBUG,
            show_level=settings.DEBUG,
            show_path=settings.DEBUG,
            rich_tracebacks=True,
            console=self.console,
        )
        log_handler.setLevel(DEBUG if settings.DEBUG else INFO)

        if settings.DEBUG:
            formatter = Formatter("[%(asctime)s] %(name)s - %(levelname)s: %(message)s")
        else:
            formatter = Formatter("%(message)s")
        log_handler.setFormatter(formatter)

        self.logger.addHandler(log_handler)

    def debug(self, msg: str) -> None:
        self.logger.debug(msg)

    def info(self, msg: str) -> None:
        self.logger.info(msg)

    def warning(self, msg: str) -> None:
        self.logger.warning(msg)

    def error(self, msg: str) -> None:
        self.logger.error(msg)

    def critical(self, msg: str) -> None:
        self.logger.critical(msg)

    def exception(self, msg: str) -> None:
        self.logger.exception(msg)

    def print(self, msg: str) -> None:
        self.console.print(msg)


================================================
File: app/core/settings.py
================================================
from rich import pretty, traceback
from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    APP_NAME: str = "LinkCovery"
    DATABASE_NAME: str = "app.db"
    DEBUG: bool = False
    ALLOW_EXTENSIONS: list = [
        ".txt",
        ".csv",
        ".json",
    ]

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"


settings = Settings()


if settings.DEBUG:
    traceback.install(show_locals=True)
    pretty.install()


================================================
File: app/core/utils.py
================================================
from os import path

from app.core.settings import settings


def check_file(file_path: str) -> bool:
    if not path.exists(file_path):
        raise FileNotFoundError(f"File not found: {file_path}")

    if (extension := path.splitext(file_path)[1].lower()) not in settings.ALLOW_EXTENSIONS:
        raise ValueError(f"Invalid file extension: {extension}. Allowed extensions: {settings.ALLOW_EXTENSIONS}")

    return True


def get_description(text: str | None) -> str:
    return text


================================================
File: app/core/database/__init__.py
================================================
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from app.core.settings import settings
from app.core.database.models import Base, User, Link
from app.core.database.crud import UserService, LinkService

engine = create_engine(f"sqlite:///{settings.DATABASE_NAME}")
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Create tables
Base.metadata.create_all(bind=engine)

# Initialize services
session = SessionLocal()
user_service = UserService(session)
link_service = LinkService(session)

__all__ = ["user_service", "link_service", "User", "Link"]


================================================
File: app/core/database/crud.py
================================================
from sqlalchemy.orm import Session
from datetime import datetime

from app.core.database.repositories import UserRepository, LinkRepository


class UserService:
    def __init__(self, session: Session):
        self.user_repository = UserRepository(session)

    def create_user(self, user_data):
        return self.user_repository.create(user_data)

    def get_user(self, user_id: int | None = None, user_email: str | None = None):
        if user_id:
            return self.user_repository.get_by_id(user_id)
        elif user_email:
            return self.user_repository.get_by_email(user_email)
        else:
            return None

    def update_user(self, user_id: int, user_data):
        return self.user_repository.update(user_id, user_data)

    def delete_user(self, user_id: int):
        return self.user_repository.delete(user_id)

    def get_users(self):
        return self.user_repository.get_all()


class LinkService:
    def __init__(self, session: Session):
        self.link_repository = LinkRepository(session)

    def create_link(self, **link_data):
        link_data["created_at"] = datetime.utcnow()
        link_data["updated_at"] = link_data["created_at"]
        return self.link_repository.create(link_data)

    def get_link(self, link_id: int | None = None, link_url: str | None = None):
        if link_id:
            return self.link_repository.get_by_id(link_id)
        elif link_url:
            return self.link_repository.get_by_url(link_url)
        else:
            return None

    def search_links(self, search_criteria):
        return self.link_repository.search(search_criteria)

    def update_link(self, link_id: int, **link_data):
        link_data["updated_at"] = datetime.utcnow()
        return self.link_repository.update(link_id, link_data)

    def delete_link(self, link_id: int):
        return self.link_repository.delete(link_id)

    def get_links(self):
        return self.link_repository.get_all()

    def get_links_by_author(self, author_id: int, number: int | None = None):
        return self.link_repository.get_links_by_author(author_id, number)


================================================
File: app/core/database/models.py
================================================
from sqlalchemy import Column, Integer, String, Boolean, ForeignKey
from sqlalchemy.orm import relationship, declarative_base, validates

Base = declarative_base()


class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, autoincrement=True)
    name = Column(String, nullable=False)
    email = Column(String, nullable=False, unique=True)

    links = relationship("Link", back_populates="author")

    @validates("name")
    def validate_name(self, key, value):
        if len(value) < 4:
            raise ValueError("Name must be at least 4 characters long.")
        return value

    @validates("email")
    def validate_email(self, key, value):
        return value


class Link(Base):
    __tablename__ = "links"

    id = Column(Integer, primary_key=True, autoincrement=True)
    url = Column(String, nullable=False)
    domain = Column(String, nullable=False)
    description = Column(String, nullable=True)
    tag = Column(String, nullable=False)
    author_id = Column(Integer, ForeignKey("users.id"), nullable=False)
    is_read = Column(Boolean, default=False)
    created_at = Column(String, nullable=False)
    updated_at = Column(String, nullable=False)

    author = relationship("User", back_populates="links")

    @validates("url")
    def validate_url(self, key, value):
        if not value.startswith("http://") and not value.startswith("https://"):
            raise ValueError("Invalid URL format.")
        return value

    @validates("domain")
    def validate_domain(self, key, value):
        if not value or len(value.strip()) == 0:
            raise ValueError("Domain cannot be empty or just whitespace.")
        if "." not in value:
            raise ValueError("Domain must contain at least one dot.")
        return value.lower()

    @validates("author_id")
    def validate_author(self, key, value):
        if not value:
            raise ValueError("Author ID is required.")
        return value


================================================
File: app/core/database/repositories.py
================================================
from sqlalchemy.orm import Session

from app.core.database.models import User, Link


class UserRepository:
    def __init__(self, session: Session):
        self.session = session

    def create(self, user_data):
        if self.get_by_email(user_data.get("email")):
            raise ValueError(f"User with email '{user_data.get('email')}' already exists.")
        user = User(**user_data)
        self.session.add(user)
        self.session.commit()
        return user

    def get_by_id(self, user_id: int):
        return self.session.query(User).filter(User.id == user_id).first()

    def get_by_email(self, email: str):
        return self.session.query(User).filter(User.email == email).first()

    def update(self, user_id: int, user_data):
        if user := self.get_by_id(user_id):
            for key, value in user_data.items():
                setattr(user, key, value)
            self.session.commit()
        return user

    def delete(self, user_id: int):
        if user := self.get_by_id(user_id):
            self.session.delete(user)
            self.session.commit()

    def get_all(self):
        return self.session.query(User).all()


class LinkRepository:
    def __init__(self, session: Session):
        self.session = session

    def create(self, link_data):
        if self.get_by_url(link_data.get("url")):
            raise ValueError(f"Link with URL '{link_data.get('url')}' already exists.")
        link = Link(**link_data)
        self.session.add(link)
        self.session.commit()
        return link

    def get_by_id(self, link_id: int):
        return self.session.query(Link).filter(Link.id == link_id).first()

    def get_by_url(self, url: str):
        return self.session.query(Link).filter(Link.url == url).first()

    def search(self, search_criteria):
        query = self.session.query(Link)
        # Filter by domain if provided
        if search_criteria.get("domain"):
            query = query.filter(Link.domain.contains(search_criteria["domain"]))
        # Filter by each tag provided
        if search_criteria.get("tag"):
            for tag in search_criteria["tag"]:
                query = query.filter(Link.tag.contains(tag))
        # Filter by description if provided
        if search_criteria.get("description"):
            query = query.filter(Link.description.contains(search_criteria["description"]))
        # Filter by read status if provided
        if search_criteria.get("is_read") is not None:
            query = query.filter(Link.is_read == search_criteria["is_read"])
        # Sorting: if a sort field is provided and exists on the Link model
        sort_by = search_criteria.get("sort_by")
        sort_order = search_criteria.get("sort_order", "ASC")
        if sort_by and hasattr(Link, sort_by):
            column = getattr(Link, sort_by)
            if sort_order.upper() == "DESC":
                query = query.order_by(column.desc())
            else:
                query = query.order_by(column.asc())
        # Pagination: apply offset and limit if provided
        if search_criteria.get("offset") is not None:
            query = query.offset(search_criteria["offset"])
        if search_criteria.get("limit") is not None:
            query = query.limit(search_criteria["limit"])
        return query.all()

    def update(self, link_id: int, link_data):
        if link := self.get_by_id(link_id):
            for key, value in link_data.items():
                setattr(link, key, value)
            self.session.commit()
        return link

    def delete(self, link_id: int):
        if link := self.get_by_id(link_id):
            self.session.delete(link)
            self.session.commit()

    def get_all(self):
        return self.session.query(Link).all()

    def get_links_by_author(self, author_id: int, number: int | None = None):
        if number:
            return self.session.query(Link).filter(Link.author_id == author_id).limit(number).all()
        return self.session.query(Link).filter(Link.author_id == author_id).all()


================================================
File: app/core/services/import_export/exporter.py
================================================
from csv import DictWriter
from json import dump
from rich.progress import track

from app.core.database import link_service, user_service
from app.core.logger import AppLogger

logger = AppLogger(__name__)


def export_users_to_json(output_path: str) -> None:
    users = user_service.get_users()
    users_data = []
    for user in users:
        users_data.append({col: getattr(user, col) for col in user.__table__.columns.keys()})
    try:
        with open(output_path, "w", encoding="utf-8") as json_file:
            dump(users_data, json_file, indent=4)
        logger.info(f"Successfully exported {len(users)} users to {output_path}.")
    except Exception as e:
        logger.error(f"Failed to export users to JSON: {e}")


def export_users_to_csv(output_path: str) -> None:
    users = user_service.get_users()
    if not users:
        logger.warning("No users available to export.")
        return

    try:
        with open(output_path, "w", newline="", encoding="utf-8") as csv_file:
            fieldnames = list(users[0].__table__.columns.keys())
            writer = DictWriter(csv_file, fieldnames=fieldnames)
            writer.writeheader()
            for user in track(users, description="Exporting users..."):
                row = {col: getattr(user, col) for col in fieldnames}
                writer.writerow(row)
        logger.info(f"Successfully exported {len(users)} users to {output_path}.")
    except Exception as e:
        logger.error(f"Failed to export users to CSV: {e}")


def export_links_to_json(output_path: str, author_id: int | None = None) -> None:
    if author_id is not None:
        links = link_service.get_links_by_author(author_id)
    else:
        links = link_service.get_links()
    links_data = []
    for link in links:
        link_data = {col: getattr(link, col) for col in link.__table__.columns.keys()}
        if link.author:
            link_data["author"] = {col: getattr(link.author, col) for col in link.author.__table__.columns.keys()}
        links_data.append(link_data)
    try:
        with open(output_path, "w", encoding="utf-8") as json_file:
            dump(links_data, json_file, indent=4)
        logger.info(f"Successfully exported {len(links_data)} links to {output_path}.")
    except Exception as e:
        logger.error(f"Failed to export links to JSON: {e}")


def export_links_to_csv(output_path: str, author_id: int | None = None) -> None:
    if author_id is not None:
        links = link_service.get_links_by_author(author_id)
    else:
        links = link_service.get_links()
    if not links:
        logger.warning("No links available to export.")
        return

    headers = list(links[0].__table__.columns.keys()) + ["author_name", "author_email"]
    try:
        with open(output_path, "w", newline="", encoding="utf-8") as csv_file:
            writer = DictWriter(csv_file, fieldnames=headers)
            writer.writeheader()
            for link in track(links, description="Exporting links..."):
                row = {col: getattr(link, col) for col in link.__table__.columns.keys()}
                if link.author:
                    row["author_name"] = link.author.name
                    row["author_email"] = link.author.email
                else:
                    row["author_name"] = ""
                    row["author_email"] = ""
                writer.writerow(row)
        logger.info(f"Successfully exported {len(links)} links to {output_path}.")
    except Exception as e:
        logger.error(f"Failed to export links to CSV: {e}")


================================================
File: app/core/services/import_export/importer.py
================================================
from pydantic import HttpUrl, ValidationError, parse_obj_as
from urllib.parse import urlparse
from csv import DictReader
from json import JSONDecodeError, load

from app.core.logger import AppLogger
from app.core.utils import get_description
from app.core.database import link_service

logger = AppLogger(__name__)


def txt_import(file_path: str, author_id: int):
    with open(file_path, "r", encoding="utf-8") as content:
        links = [line.strip() for line in content if line.strip()]
        if not links:
            logger.info("No links found in the TXT file.")
            return

    added_link = 0
    for line_number, link in enumerate(links, start=1):
        try:
            url = str(parse_obj_as(HttpUrl, link))
            domain = urlparse(url).netloc
            tags = ", ".join(domain.split("."))
            link_service.create_link(
                url=url,
                description=get_description(None),
                domain=domain,
                tag=tags,
                author_id=author_id,
            )
            added_link += 1
        except (ValidationError, Exception) as e:
            logger.error(f"Failed to add link at line {line_number}. Error: {e}")

    logger.info(f"Successfully imported {added_link} links from TXT file for user {author_id}.")


def csv_import(file_path: str, author_id: int):
    with open(file_path, "r", encoding="utf-8") as content:
        reader = DictReader(content)
        if not reader.fieldnames:
            logger.info("CSV file is empty or invalid.")
            return

        required_fields = {"url", "domain", "description", "tag", "is_read"}
        if not required_fields.issubset(reader.fieldnames):
            logger.info(f"CSV file is missing required fields. Required fields: {required_fields}")
            return

        links = list(reader)
        if not links:
            logger.info("No links found in the CSV file.")
            return

    added_link = 0
    for line_number, row in enumerate(links, start=2):
        try:
            url = str(parse_obj_as(HttpUrl, row["url"]))
            domain = row.get("domain") or urlparse(url).netloc
            tags = row.get("tag") or ", ".join(domain.split("."))
            is_read = str(row.get("is_read", "False")).strip().lower() in {"1", "true", "yes"}
            link_service.create_link(
                url=url,
                description=get_description(row.get("description")),
                domain=domain,
                tag=tags,
                author_id=author_id,
                is_read=is_read,
            )
            added_link += 1
        except (ValidationError, Exception) as e:
            logger.error(f"Failed to add link at line {line_number}. Error: {e}")

    logger.info(f"Successfully imported {added_link} links from CSV file for user {author_id}.")


def json_import(file_path: str, author_id: int):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            links_data = load(f)
    except JSONDecodeError as e:
        logger.error(f"Invalid JSON format: {e}")
        return
    except Exception as e:
        logger.error(f"Failed to open JSON file: {e}")
        return

    if not links_data:
        logger.info("No links found in the JSON file.")
        return

    added_link = 0
    for index, link_dict in enumerate(links_data, start=1):
        try:
            url = str(parse_obj_as(HttpUrl, link_dict["url"]))
            domain = link_dict.get("domain") or urlparse(url).netloc
            tags = link_dict.get("tag") or ", ".join(domain.split("."))
            description = get_description(link_dict.get("description"))
            is_read = link_dict.get("is_read", False)
            link_service.create_link(
                url=url,
                description=description,
                domain=domain,
                tag=tags,
                author_id=author_id,
                is_read=is_read,
            )
            added_link += 1
        except (ValidationError, Exception) as e:
            logger.error(f"Failed to add link at index {index} from JSON. Error: {e}")

    logger.info(f"Successfully imported {added_link} links from JSON file for user {author_id}.")
